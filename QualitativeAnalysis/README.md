# Qualitative Analysis
We provide the actual files and guidelines used for the analysis in our paper.

## Human Evaluation
For each dataset, we randomly selected 30 samples for human evaluation. To reduce bias according to the number of triplets, we set intervals (1-3 triplets, 4-6 triplets, over 7 triplets) and sampled 10 samples from each interval.

Each dataset was provided to the evaluator in a blinded manner, represented numerically (e.g., 1, 2, 3, 4, 5).

```
1.pdf #Genwiki
2.pdf #LAGRANGE
3.pdf #TekGEN
4.pdf #WikiOFGraph
5.pdf #WebNLG
```

Results are saved as csv format in ```results``` divided by ```evaluator #```.


## GPT Evaluation
For each dataset, we randomly selected 1000 samples for GPT evaluation. To reduce bias according to the number of triplets, we set intervals (1-3 triplets, 4-6 triplets, over 7 triplets) and sampled 334,333,333 samples from each interval.

We provide actual prompt and code for GPT-4o evaluation in ```gpteval.py```.

Each dataset was provided to GPT using the same sampling parameters, and the results were separately stored.

The original datasets used for evaluation are stored in the ```source``` directory, while the prompt results generated by GPT are stored in the ```results``` directory.

```
1 #Genwiki
2 #LAGRANGE
3 #TekGEN
4 #WikiOFGraph
5 #WebNLG
```